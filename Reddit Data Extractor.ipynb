{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 :: Community Survy',\n",
       " '60 :: Flarum \\xe2\\x80\\x93 Free, open-source forum software with a focus on simplicity.',\n",
       " '0 :: Platform cooperativism as a critique of open-source',\n",
       " '1 :: Open source software communities?',\n",
       " '8 :: Guy goes to Eurovision, releases hundreds of freely licensed images']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw\n",
    "r = praw.Reddit(user_agent='/r/Analog_Scraper by /u/compassdestroyer')\n",
    "submissions = r.get_subreddit('opensource').get_hot(limit=5)\n",
    "[str(x) for x in submissions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_agent = \"/r/Analog_Scraper by /u/compassdestroyer\"\n",
    "r = praw.Reddit(user_agent=user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#items.py\n",
    "from scrapy import Item, Field\n",
    "\n",
    "class RedditItem(Item):\n",
    "    subreddit = Field()\n",
    "    link = Field()\n",
    "    title = Field()\n",
    "    authors = Field()\n",
    "    date = Field()\n",
    "    vote = Field()\n",
    "    #top_comment = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from scrapy import Spider, Request\n",
    "from items import RedditItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RedditSpider(Spider):\n",
    "    name = 'reddit'\n",
    "    allowed_domains = ['reddit.com']\n",
    "    start_urls = ['https://www.reddit.com/r/circlejerk',\n",
    "                  'https://www.reddit.com/r/gaming',\n",
    "                  'https://www.reddit.com/r/floridaman',\n",
    "                  'https://www.reddit.com/r/movies',\n",
    "                  'https://www.reddit.com/r/science',\n",
    "                  'https://www.reddit.com/r/seahawks',\n",
    "                  'https://www.reddit.com/r/totallynotrobots',\n",
    "                  'https://www.reddit.com/r/uwotm8',\n",
    "                  'https://www.reddit.com/r/videos',\n",
    "                  'https://www.reddit.com/r/worldnews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(self, response): \n",
    "    links = response.xpath('//p[@class=\"title\"]/a[@class=\"title may-blank \"]/@href').extract()\n",
    "    titles = response.xpath('//p[@class=\"title\"]/a[@class=\"title may-blank \"]/text()').extract()\n",
    "    dates = response.xpath('//p[@class=\"tagline\"]/time[@class=\"live-timestamp\"]/@title').extract()\n",
    "    authors = response.xpath('//p[@class=\"tagline\"]/a[@class=\"author may-blank \"]/text').extract()\n",
    "    votes = response.xpath('//div[@class=\"midcol unvoted\"]/div[@class=\"score unvoted\"]/text()').extract()\n",
    "    #comments = response.xpath('//div[@id=\"siteTable\"]//a[@class=\"comments may-blank\"]/@href').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#settings.py\n",
    "BOT_NAME = 'reddit'\n",
    "SPIDER_MODULES = ['reddit.spiders']\n",
    "NEWSPIDER_MODULE = 'reddit.spiders'\n",
    "\n",
    "DOWNLOAD_DELAY = 2\n",
    "\n",
    "ITEM_PIPELINES = {'reddit.pipelines.DuplicatesPipeline':300, 'reddit.pipelines.MongoDBPipeline':800, }\n",
    "\n",
    "MONGODB_SERVER = \"localhost\"\n",
    "MONGODB_PORT = 27017\n",
    "MONGODB_DB = \"reddit\"\n",
    "MONGODB_COLLECTION = \"post\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zlandes/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: ScrapyDeprecationWarning: Module `scrapy.conf` is deprecated, use `crawler.settings` attribute instead\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/zlandes/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: ScrapyDeprecationWarning: Module `scrapy.log` has been deprecated, Scrapy now relies on the builtin Python library for logging. Read the updated logging entry in the documentation to learn more.\n"
     ]
    }
   ],
   "source": [
    "import pymongo \n",
    "from scrapy.conf import settings \n",
    "from scrapy.exceptions import DropItem \n",
    "from scrapy import log \n",
    "\n",
    "class DuplicatesPipeline(object):\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.ids_seen = set()\n",
    "        \n",
    "    def process_item(self, item, spider):\n",
    "        if item['link'] in self.ids_seen:\n",
    "            raise DropItem(\"Duplicate item found: %s\" % item) \n",
    "        else: \n",
    "            self.ids_seen.add(item['link']) \n",
    "            return item \n",
    "class MongoDBPipeline(object):\n",
    "    def __init__(self): \n",
    "        connection = pymongo.MongoClient(\n",
    "            settings['MONGODB_SERVER'],\n",
    "            settings['MONGODB_PORT'] \n",
    "        ) \n",
    "        db = connection[settings['MONGODB_DB']]\n",
    "        self.collection = db[settings['MONGODB_COLLECTION']]\n",
    "        \n",
    "    def process_item(self, item, spider):\n",
    "        valid = True \n",
    "        for data in item:\n",
    "            if not data: \n",
    "                valid = False\n",
    "                raise DropItem(\"Missing {0}!\".format(data))\n",
    "        if valid: \n",
    "            self.collection.insert(dict(item))\n",
    "            log.msg(\"Added to MongoDB database!\",\n",
    "                    level=log.DEBUG, spider=spider) \n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
